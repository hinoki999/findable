# DropLink Production Testing Suite

Comprehensive testing infrastructure for the DropLink mobile app to catch OTA deployment issues and backend persistence problems.

## ğŸ“ Structure

```
testing/
â”œâ”€â”€ backend-tester.py       # Backend API endpoint tests
â”œâ”€â”€ database-validator.py   # Direct PostgreSQL database validation
â”œâ”€â”€ ota-monitor.js          # GitHub Actions & EAS update monitoring
â”œâ”€â”€ integration-tester.js   # End-to-end user flow testing
â”œâ”€â”€ run-all-tests.ps1       # Master test orchestrator (PowerShell)
â”œâ”€â”€ logs/                   # Individual test logs (auto-created)
â””â”€â”€ ERRORS.log             # Consolidated error log
```

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.x** (for backend-tester.py and database-validator.py)
   ```powershell
   pip install requests psycopg2-binary
   ```

2. **Node.js** (for ota-monitor.js and integration-tester.js)
   ```powershell
   # Node.js includes https module by default - no install needed
   ```

3. **PowerShell** (for run-all-tests.ps1)
   - Pre-installed on Windows

### Environment Setup

Set these environment variables (optional but recommended):

```powershell
# GitHub token (for Actions monitoring)
$env:GITHUB_TOKEN = "ghp_your_github_token_here"

# Expo token (for EAS monitoring)
$env:EXPO_TOKEN = "your_expo_token_here"

# Database URL (for direct DB validation)
$env:DATABASE_URL = "postgresql://user:pass@host:port/dbname"
```

### Running Tests

**Option 1: Run all tests continuously (recommended)**
```powershell
cd testing
.\run-all-tests.ps1
```

**Option 2: Run individual tests**
```powershell
# Backend API tests
python backend-tester.py

# Database validation
python database-validator.py

# OTA monitoring
node ota-monitor.js

# Integration tests
node integration-tester.js
```

## ğŸ“Š What Each Test Does

### 1. Backend API Tests (`backend-tester.py`)
- âœ… Tests POST /auth/signup
- âœ… Tests POST /auth/login
- âœ… Tests GET /user/profile (includes hasCompletedOnboarding)
- âœ… Tests POST /user/profile (updates hasCompletedOnboarding)
- âœ… Validates data persistence across requests

### 2. Database Validation (`database-validator.py`)
- âœ… Directly queries Railway PostgreSQL
- âœ… Checks user_profiles table for test user
- âœ… Validates has_completed_onboarding column
- âœ… Logs when values change (or fail to change)

### 3. OTA Monitor (`ota-monitor.js`)
- âœ… Polls GitHub Actions API for workflow status
- âœ… Checks "OTA Update on Push" workflow runs
- âœ… Verifies workflow success/failure
- âœ… Monitors EAS update deployments
- âœ… Validates runtime version (1.0.1)

### 4. Integration Tests (`integration-tester.js`)
- âœ… **Signup Flow**: Create user â†’ Set onboarding flag â†’ Verify persistence
- âœ… **Login Flow**: Login existing user â†’ Check onboarding status
- âœ… **Tutorial Completion**: Reset flag â†’ Complete tutorials â†’ Verify backend update

## ğŸ¯ Key Features

### Continuous Monitoring
- Runs all tests every 60 seconds by default
- Catches issues as they happen in production
- No manual intervention required

### Color-Coded Output
- ğŸŸ¢ Green = Tests passed
- ğŸ”´ Red = Tests failed
- ğŸŸ¡ Yellow = Warnings or in-progress

### Comprehensive Logging
- Individual log files per test in `logs/` directory
- Consolidated `ERRORS.log` for all failures
- Timestamps on every log entry

### Production-Ready
- Tests against live Railway backend
- Monitors real GitHub Actions workflows
- Validates actual EAS deployments
- Uses real user accounts and data

## ğŸ”§ Configuration

Edit the configuration at the top of each file:

**backend-tester.py & integration-tester.js:**
```python
BASE_URL = "https://findable-production.up.railway.app"
TEST_USER = "caitie690"
TEST_PASSWORD = "your_password_here"  # Update this!
```

**database-validator.py:**
```python
DATABASE_URL = os.environ.get('DATABASE_URL', '')
TEST_USER = "caitie690"
```

**ota-monitor.js:**
```javascript
githubRepo: 'hinoki999/findable',
easProject: '@hirule/mobile',
easBranch: 'preview',
runtimeVersion: '1.0.1'
```

**run-all-tests.ps1:**
```powershell
$TestInterval = 60  # Seconds between test runs
```

## ğŸ“ Common Issues & Solutions

### "âŒ Login failed"
- Update `TEST_PASSWORD` in backend-tester.py and integration-tester.js
- Verify user exists in database

### "âŒ DATABASE_URL not set"
- Get connection string from Railway dashboard
- Set environment variable: `$env:DATABASE_URL='postgresql://...'`

### "âš ï¸ GITHUB_TOKEN not set"
- Create token at: https://github.com/settings/tokens
- Set environment variable: `$env:GITHUB_TOKEN='ghp_...'`

### "âš ï¸ EXPO_TOKEN not set"
- Get token from: https://expo.dev/accounts/[account]/settings/access-tokens
- Set environment variable: `$env:EXPO_TOKEN='...'`

## ğŸ¯ What We're Testing For

These tests specifically catch the issues you're experiencing:

1. **Backend Persistence Issues**
   - Does `hasCompletedOnboarding` actually save to database?
   - Does it persist across logout/login cycles?
   - Does it sync across devices?

2. **OTA Update Deployment**
   - Do pushes to `develop` trigger GitHub Actions?
   - Does the workflow publish to EAS correctly?
   - Are updates reaching the `preview` branch?
   - Is the runtime version correct (1.0.1)?

3. **Tutorial System Integration**
   - Does signup set the onboarding flag?
   - Does tutorial completion update the backend?
   - Does login skip tutorials for existing users?

## ğŸ“ˆ Success Metrics

All tests passing means:
- âœ… Backend API is responding correctly
- âœ… Database is storing data properly
- âœ… OTA updates are deploying successfully
- âœ… Tutorial system is working end-to-end
- âœ… User data persists across sessions

## ğŸš¨ When Tests Fail

Check `ERRORS.log` for detailed error messages. Common failure patterns:

- **All backend tests failing** â†’ Backend server issue
- **Database validation failing** â†’ Data not persisting to DB
- **OTA monitor failing** â†’ GitHub Actions or EAS deployment issue
- **Integration tests failing** â†’ End-to-end flow broken

## ğŸ’¡ Tips

1. **Run tests before pushing code** - Catch issues early
2. **Leave tests running** - Continuous monitoring catches intermittent issues
3. **Check logs after failed deployments** - See what broke
4. **Monitor ERRORS.log** - One place for all failures

## ğŸ‰ Next Steps

Once tests are passing consistently:
1. Add more test cases for edge cases
2. Set up automated alerts (email/Slack)
3. Integrate with CI/CD pipeline
4. Add performance benchmarks

---

**Happy Testing! ğŸš€**

If you find bugs the tests aren't catching, add new test cases to improve coverage.

